"use strict";(self.webpackChunkdagger=self.webpackChunkdagger||[]).push([[492],{3905:(e,a,t)=>{t.d(a,{Zo:()=>p,kt:()=>m});var n=t(7294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=n.createContext({}),u=function(e){var a=n.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},p=function(e){var a=u(e.components);return n.createElement(l.Provider,{value:a},e.children)},d={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},c=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),c=u(t),m=r,g=c["".concat(l,".").concat(m)]||c[m]||d[m]||o;return t?n.createElement(g,i(i({ref:a},p),{},{components:t})):n.createElement(g,i({ref:a},p))}));function m(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=c;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var u=2;u<o;u++)i[u]=t[u];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}c.displayName="MDXCreateElement"},1250:(e,a,t)=>{t.r(a),t.d(a,{contentTitle:()=>i,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var n=t(7462),r=(t(7294),t(3905));const o={},i="Create a job",s={unversionedId:"guides/create_dagger",id:"guides/create_dagger",isDocsHomePage:!1,title:"Create a job",description:"This page contains how-to guides for creating a Dagger job and configure it.",source:"@site/docs/guides/create_dagger.md",sourceDirName:"guides",slug:"/guides/create_dagger",permalink:"/dagger/docs/guides/create_dagger",editUrl:"https://github.com/odpf/dagger/edit/master/docs/docs/guides/create_dagger.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Choosing a Source",permalink:"/dagger/docs/guides/choose_source"},next:{title:"Example Queries",permalink:"/dagger/docs/guides/query_examples"}},l=[{value:"Prerequisites",id:"prerequisites",children:[{value:"<code>JDK and Gradle</code>",id:"jdk-and-gradle",children:[]},{value:"<code>A Source</code>",id:"a-source",children:[]}]},{value:"Common Configurations",id:"common-configurations",children:[]},{value:"Write SQL",id:"write-sql",children:[]},{value:"Log Sink",id:"log-sink",children:[]},{value:"Influx Sink",id:"influx-sink-1",children:[]},{value:"Kafka Sink",id:"kafka-sink",children:[]},{value:"Bigquery Sink",id:"bigquery-sink",children:[{value:"BigQuery Sink Features:",id:"bigquery-sink-features",children:[]}]},{value:"Advanced Data Processing",id:"advanced-data-processing",children:[]}],u={toc:l};function p(e){let{components:a,...t}=e;return(0,r.kt)("wrapper",(0,n.Z)({},u,t,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"create-a-job"},"Create a job"),(0,r.kt)("p",null,"This page contains how-to guides for creating a Dagger job and configure it."),(0,r.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,r.kt)("p",null,"Dagger is a stream processing framework built with Apache Flink to process/aggregate/transform protobuf data. To run a dagger in any environment you need to have the following things set up beforehand."),(0,r.kt)("h3",{id:"jdk-and-gradle"},(0,r.kt)("inlineCode",{parentName:"h3"},"JDK and Gradle")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Java 1.8 and gradle(5+) need to be installed to run in local mode. Follow this ",(0,r.kt)("a",{parentName:"li",href:"https://www.oracle.com/in/java/technologies/javase/javase-jdk8-downloads.html"},"link")," to download Java-1.8 in your setup and ",(0,r.kt)("a",{parentName:"li",href:"https://gradle.org/install/"},"this")," to set up gradle.")),(0,r.kt)("h3",{id:"a-source"},(0,r.kt)("inlineCode",{parentName:"h3"},"A Source")),(0,r.kt)("p",null,"Dagger currently supports 3 kinds of Data Sources. Here are the requirements for each:"),(0,r.kt)("h5",{id:"kafka_source-and-kafka_consumer"},(0,r.kt)("inlineCode",{parentName:"h5"},"KAFKA_SOURCE")," and ",(0,r.kt)("inlineCode",{parentName:"h5"},"KAFKA_CONSUMER")),(0,r.kt)("p",null,"Both these sources use ",(0,r.kt)("a",{parentName:"p",href:"https://kafka.apache.org/"},"Kafka")," as the source of data. So you need to set up Kafka(1.0+) either\nin a local or clustered environment. Follow this ",(0,r.kt)("a",{parentName:"p",href:"https://kafka.apache.org/quickstart"},"quick start")," to set up Kafka in\nthe local machine. If you have a clustered Kafka you can configure it to use in Dagger directly."),(0,r.kt)("h5",{id:"parquet_source"},(0,r.kt)("inlineCode",{parentName:"h5"},"PARQUET_SOURCE")),(0,r.kt)("p",null,"This source uses Parquet files as the source of data. The parquet files can be either hourly partitioned, such as"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"root_folder\n    - booking_log\n        - dt=2022-02-05\n            - hr=09\n                * g6agdasgd6asdgvadhsaasd829ajs.parquet\n                * . . . (more parquet files)\n            - (...more hour folders)\n        - (... more date folders)\n\n")),(0,r.kt)("p",null,"or date partitioned, such as:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-text"},"root_folder\n    - shipping_log\n        - dt=2021-01-11\n            * hs7hasd6t63eg7wbs8swssdasdasdasda.parquet\n            * ...(more parquet files)\n        - (... more date folders)\n\n")),(0,r.kt)("p",null,"The file paths can be either in the local file system or in GCS bucket. When parquet files are provided from GCS bucket,\nDagger will require a ",(0,r.kt)("inlineCode",{parentName:"p"},"core_site.xml")," to be configured in order to connect and read from GCS. A sample ",(0,r.kt)("inlineCode",{parentName:"p"},"core_site.xml")," is\npresent in dagger and looks like this:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-xml"},"<configuration>\n  <property>\n    <name>google.cloud.auth.service.account.enable</name>\n    <value>true</value>\n  </property>\n  <property>\n    <name>google.cloud.auth.service.account.json.keyfile</name>\n    <value>/Users/dummy/secrets/google_service_account.json</value>\n  </property>\n  <property>\n    <name>fs.gs.requester.pays.mode</name>\n    <value>CUSTOM</value>\n    <final>true</final>\n  </property>\n  <property>\n    <name>fs.gs.requester.pays.buckets</name>\n    <value>my_sample_bucket_name</value>\n    <final>true</final>\n  </property>\n  <property>\n    <name>fs.gs.requester.pays.project.id</name>\n    <value>my_billing_project_id</value>\n    <final>true</final>\n  </property>\n</configuration>\n")),(0,r.kt)("p",null,"You can look into the official ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/CONFIGURATION.md"},"GCS Hadoop Connectors"),"\ndocumentation to know more on how to edit this xml as per your needs."),(0,r.kt)("h4",{id:"flink-optional"},(0,r.kt)("inlineCode",{parentName:"h4"},"Flink [optional]")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Dagger uses ",(0,r.kt)("a",{parentName:"p",href:"https://flink.apache.org/"},"Apache Flink")," as the underlying framework for distributed stream processing. The current version of Dagger uses ",(0,r.kt)("a",{parentName:"p",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.9/"},"Flink-1.9"),".")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"For distributed stream processing Flink could be configured either on a standalone cluster or in one of the supported resource managers(like Yarn, Kubernetes or docker-compose). To know more about deploying Flink on production read the ",(0,r.kt)("a",{parentName:"p",href:"/dagger/docs/guides/deployment"},"deployment section"),".")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"If you want to run dagger in the local machine on a single process, you don't need to install Flink. The following Gradle command should be able to do so."))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"$ ./gradlew dagger-core:runFlink\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"To run the Flink jobs in the local machine with java jar and local properties run the following commands.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"# Creating a fat jar\n$ ./gradlew :dagger-core:fatJar\n\n# Running the jvm process\n$ java -jar dagger-core/build/libs/dagger-core-<dagger-version>-fat.jar ConfigFile=<filepath>\n")),(0,r.kt)("h4",{id:"protobuf-schema"},(0,r.kt)("inlineCode",{parentName:"h4"},"Protobuf Schema")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Dagger exclusively supports ",(0,r.kt)("a",{parentName:"li",href:"https://developers.google.com/protocol-buffers"},"protobuf")," encoded data. That is, for a\nsource reading from Kafka, Dagger consumes protobuf data from Kafka topics and does the processing. For a source reading\nfrom Parquet Files, dagger uses protobuf schema to parse the Row Group. When pushing the results to a sink, Dagger produces\ndata as per the output protobuf schema to a Kafka topic(when the sink is Kafka)."),(0,r.kt)("li",{parentName:"ul"},"When using Kafka as a source, you can push data to a Kafka topic as per protobuf format using any of the Kafka client\nlibraries. You can follow this ",(0,r.kt)("a",{parentName:"li",href:"https://www.conduktor.io/how-to-produce-and-consume-protobuf-records-in-apache-kafka/"},"tutorial"),"."),(0,r.kt)("li",{parentName:"ul"},"For all kinds of sources, you need to define the\n",(0,r.kt)("a",{parentName:"li",href:"https://developers.google.com/protocol-buffers/docs/javatutorial"},"java compiled protobuf schema")," in the classpath or\nuse our in-house schema registry tool like ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/odpf/stencil"},"Stencil")," to let dagger know about the data\nschema. Stencil is an event schema registry that provides an abstraction layer for schema handling, schema caching, and\ndynamic schema updates. ",(0,r.kt)("a",{parentName:"li",href:"/dagger/docs/reference/configuration#schema-registry"},"These configurations")," needs to be set if you are\nusing stencil for proto schema handling.")),(0,r.kt)("h4",{id:"sinks"},(0,r.kt)("inlineCode",{parentName:"h4"},"Sinks")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"The current version of dagger supports Log, BigQuery, InfluxDB and Kafka as supported sinks to push the data after processing. You need to set up the desired sinks beforehand so that data can be pushed seamlessly."),(0,r.kt)("h5",{parentName:"li",id:"influx-sink"},(0,r.kt)("inlineCode",{parentName:"h5"},"Influx Sink")),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://github.com/influxdata/influxdb"},"InfluxDB")," is an open-source time-series database with a rich ecosystem. Influx sink allows users to analyze the data in real-time. With tools like ",(0,r.kt)("a",{parentName:"li",href:"https://grafana.com/"},"Grafana"),", we can even visualize the data and create real-time dashboards to monitor different metrics."),(0,r.kt)("li",{parentName:"ul"},"You need to set up an Opensourced version of InfluxDB to use it as a sink. Follow ",(0,r.kt)("a",{parentName:"li",href:"https://docs.influxdata.com/influxdb/v1.8/introduction/install/"},"this")," to get started with the influx. We currently support InfluxDB-1.0+ in the dagger."),(0,r.kt)("li",{parentName:"ul"},"If you want to visualize the data pushed to influxDB, you have to set up grafana and add the configured influxDB as a data source.")),(0,r.kt)("h5",{parentName:"li",id:"kafka-sink-"},(0,r.kt)("inlineCode",{parentName:"h5"},"Kafka Sink")," :"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"With Kafka sink dagger pushes the processed data as protobuf to a Kafka topic."),(0,r.kt)("li",{parentName:"ul"},"If you have a Kafka cluster set up you are good to run a Kafka sink dagger. Enable auto topic creation in Kafka or create a Kafka topic beforehand to push the data.")),(0,r.kt)("h5",{parentName:"li",id:"bigquery-sink-"},(0,r.kt)("inlineCode",{parentName:"h5"},"BigQuery Sink")," :"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"BigQuery is a fully managed enterprise data warehouse that helps you manage and analyze your data with built-in features like machine learning, geospatial analysis, and business intelligence.BigQuery's serverless architecture lets you use SQL queries to answer your organization's biggest questions with zero infrastructure management. BigQuery's scalable, distributed analysis engine lets you query terabytes in seconds and petabytes in minutes."),(0,r.kt)("li",{parentName:"ul"},"Bigquery Sink is created using the ODPF Depot library. "),(0,r.kt)("li",{parentName:"ul"},"Depot is a sink connector, which acts as a bridge between data processing systems and real sink. You can check out the Depot Github repository ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/odpf/depot/tree/main/docs"},"here"),".")))),(0,r.kt)("h2",{id:"common-configurations"},"Common Configurations"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"These configurations are mandatory for dagger creation and are sink independent. Here you need to set configurations such as the source details, the protobuf schema class, the SQL query to be applied on the streaming data, etc. In local execution, they would be set inside ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/odpf/dagger/blob/main/dagger-core/env/local.properties"},(0,r.kt)("inlineCode",{parentName:"a"},"local.properties"))," file. In the clustered environment they can be passed as job parameters to the Flink exposed job creation API."),(0,r.kt)("li",{parentName:"ul"},"Configuration for a given schema involving a single source is consolidated as a Stream. In daggers, you can configure one or more streams for a single job. To know how to configure a stream based on a source, check ",(0,r.kt)("a",{parentName:"li",href:"/dagger/docs/reference/configuration#streams"},"here")),(0,r.kt)("li",{parentName:"ul"},"The ",(0,r.kt)("inlineCode",{parentName:"li"},"FLINK_JOB_ID")," defines the name of the flink job. ",(0,r.kt)("inlineCode",{parentName:"li"},"ROWTIME_ATTRIBUTE_NAME")," is the key name of ",(0,r.kt)("a",{parentName:"li",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.13/docs/dev/table/concepts/time_attributes/"},"row time attribute")," required for stream processing."),(0,r.kt)("li",{parentName:"ul"},"In clustered mode, you can set up the ",(0,r.kt)("inlineCode",{parentName:"li"},"parallelism")," configuration for distributed processing."),(0,r.kt)("li",{parentName:"ul"},"Read more about the mandatory configurations ",(0,r.kt)("a",{parentName:"li",href:"/dagger/docs/reference/configuration"},"here"),".")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-properties"},'STREAMS = [{\n        "SOURCE_KAFKA_TOPIC_NAMES": "test-topic",\n        "INPUT_SCHEMA_TABLE": "data_stream",\n        "INPUT_SCHEMA_PROTO_CLASS": "com.tests.TestMessage",\n        "INPUT_SCHEMA_EVENT_TIMESTAMP_FIELD_INDEX": "41",\n        "SOURCE_KAFKA_CONSUMER_CONFIG_BOOTSTRAP_SERVERS": "localhost:9092",\n        "SOURCE_KAFKA_CONSUMER_CONFIG_AUTO_COMMIT_ENABLE": "false",\n        "SOURCE_KAFKA_CONSUMER_CONFIG_AUTO_OFFSET_RESET": "latest",\n        "SOURCE_KAFKA_CONSUMER_CONFIG_GROUP_ID": "dummy-consumer-group",\n        "SOURCE_KAFKA_NAME": "local-kafka-stream",\n        "SOURCE_DETAILS": [\n            {\n                "SOURCE_TYPE": "UNBOUNDED",\n                "SOURCE_NAME": "KAFKA_CONSUMER"\n            }],\n        }]\n\nFLINK_ROWTIME_ATTRIBUTE_NAME=rowtime\nFLINK_JOB_ID=TestDagger\n')),(0,r.kt)("h2",{id:"write-sql"},"Write SQL"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Dagger is built keeping SQL first philosophy in mind. Though there are other ways to define data processing in daggers like transformers and processors, SQL is always required in the dagger. You can define the SQL query on the streaming table as part of a configuration. This example is a sample SELECT star query on the about data source.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-properties"},"SQL_QUERY= SELECT COUNT(1) AS no_of_events, sample_field, TUMBLE_END(rowtime, INTERVAL '60' SECOND) AS window_timestamp FROM `test-table` GROUP BY TUMBLE (rowtime, INTERVAL '60' SECOND), sample_field\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Flink has really good native support for SQL. All the Flink supported ",(0,r.kt)("a",{parentName:"li",href:"https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/table/sql.html"},"SQL statements")," should be available out of the box for a dagger."),(0,r.kt)("li",{parentName:"ul"},"Also you can define custom User Defined Functions(UDFs) to add your SQL logic. We have some pre-supported ",(0,r.kt)("a",{parentName:"li",href:"/dagger/docs/guides/use_udf"},"UDFs")," as defined ",(0,r.kt)("a",{parentName:"li",href:"/dagger/docs/reference/udfs"},"here"),". Follow ",(0,r.kt)("a",{parentName:"li",href:"/dagger/docs/contribute/add_udf"},"this")," to add your UDFs to the dagger."),(0,r.kt)("li",{parentName:"ul"},"We have noted some of the sample queries ",(0,r.kt)("a",{parentName:"li",href:"/dagger/docs/guides/query_examples"},"here")," for different use cases.")),(0,r.kt)("h2",{id:"log-sink"},"Log Sink"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Dagger provides a log sink which simply logs the Processed Data in a readable format. A log sink dagger requires the following config to be set.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-properties"},"SQL_QUERY= SELECT * from `test-table`\nSINK=log\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Log sink is mostly used for testing and debugging purpose since it just a print statement for the processed data. This is a sample message produced in the log sink after the simple query above.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"INFO  io.odpf.dagger.core.sink.log.LogSink                            - {sample_field=81179979,sample_field_2=81179979, rowtime=2021-05-21 11:55:33.0, event_timestamp=1621598133,0}\n")),(0,r.kt)("h2",{id:"influx-sink-1"},"Influx Sink"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Influx sink is useful for data analytics or even for data validation while iterating over the business logic."),(0,r.kt)("li",{parentName:"ul"},"For the InfluxDB sink, you have to specify the InfluxDB level information as well as an influx measurement to push the processed data."),(0,r.kt)("li",{parentName:"ul"},"Showing some of the configurations essential for influx sink Dagger. Find more about them ",(0,r.kt)("a",{parentName:"li",href:"/dagger/docs/reference/configuration"},"here"),".")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-properties"},"# === sink config ===\nSINK_TYPE=influx\n# === Influx config ===\nINFLUX_URL=http://localhost:8086\nINFLUX_USERNAME=root\nINFLUX_PASSWORD=\nINFLUX_DATABASE=DAGGERS\nINFLUX_MEASUREMENT_NAME=concurrent_test\nINFLUX_BATCH_SIZE=100\nINFLUX_FLUSH_DURATION_IN_MILLISECONDS=1000\nINFLUX_RETENTION_POLICY=autogen\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"In Influx sink, add ",(0,r.kt)("a",{parentName:"p",href:"https://docs.influxdata.com/influxdb/v1.8/concepts/glossary/#tag-key"},(0,r.kt)("inlineCode",{parentName:"a"},"tag_"))," prefix to use the column as a dimension. Dimensions will help you slice the data in InfluxDB-Grafana. InfluxDB tags are essentially the columns on which data are indexed. DO NOT use tag",(0,r.kt)("em",{parentName:"p"}," for high cardinal data points with a lot of unique values unless you provide a filtering condition; this will create tag explosion & kill the InfluxDB. Ensure there is at least one value field present in the query(not starting with tag"),").")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"The data generated can be visualized with the help of tools like Grafana & chronograph."))),(0,r.kt)("h2",{id:"kafka-sink"},"Kafka Sink"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Kafka sink enables aggregated data to be published to a Kafka topic in protobuf encoded format and is available for consumption by any type of consumer. After a Dagger job is deployed, the new topic is automatically created in the output Kafka cluster if auto-creation is enabled."),(0,r.kt)("li",{parentName:"ul"},"Listing some of the configurations essential for Kafka sink Dagger. Find more about them ",(0,r.kt)("a",{parentName:"li",href:"/dagger/docs/reference/configuration"},"here"),".")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-properties"},"# === sink config ===\nSINK_TYPE=kafka\n# === Output Kafka config ===\nOUTPUT_PROTO_MESSAGE=com.tests.TestMessage\nOUTPUT_KAFKA_BROKER=localhost:9092\nOUTPUT_KAFKA_TOPIC=test-kafka-output\n")),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Dimensions & metrics from the SELECT section in the query need to be mapped to field names in the output proto."),(0,r.kt)("li",{parentName:"ul"},"Find more examples on Kafka sink SQL queries ",(0,r.kt)("a",{parentName:"li",href:"/dagger/docs/guides/query_examples#kafka-sink"},"here"),".")),(0,r.kt)("h2",{id:"bigquery-sink"},"Bigquery Sink"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"BigQuery is a data warehouse capable of quickly running SQL queries over large datasets. "),(0,r.kt)("li",{parentName:"ul"},"Bigquery Sink is created using the ODPF Depot library. Depot is a sink connector, which acts as a bridge between data processing systems and real sink. "),(0,r.kt)("li",{parentName:"ul"},"You can check out the BigQuery Sink Connector in the Depot Github repository ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md"},"here"),".")),(0,r.kt)("h3",{id:"bigquery-sink-features"},"BigQuery Sink Features:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#datatype-protobuf"},"Datatype Protobuf"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#datatype-json"},"Datatype JSON"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#bigquery-table-schema-update"},"Bigquery Table Schema Update"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#protobuf---bigquery-table-type-mapping"},"Protobuf - Bigquery Table Type Mapping"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#partitioning"},"Partitioning"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#metadata"},"Metadata"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#default-columns-for-json-data-type"},"Default columns for json data type"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#errors-handling"},"Errors Handling"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://github.com/odpf/depot/blob/main/docs/sinks/bigquery.md#google-cloud-bigquery-iam-permission"},"Google Cloud Bigquery IAM Permission")))),(0,r.kt)("h2",{id:"advanced-data-processing"},"Advanced Data Processing"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Dagger's inbuilt SQL enables users to do some complex stream processing like aggregations, joins, windowing, union etc. Find more of the sample queries ",(0,r.kt)("a",{parentName:"li",href:"/dagger/docs/guides/query_examples"},"here"),"."),(0,r.kt)("li",{parentName:"ul"},"Dagger supports an internal framework called processors to support some of the more complex Data processing beyond SQL. Some of them are,",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Custom Java code injection ",(0,r.kt)("a",{parentName:"li",href:"/dagger/docs/guides/use_transformer"},"Transformers")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/dagger/docs/advance/post_processor#external-post-processor"},"External source support")," for data enrichment"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"/dagger/docs/advance/longbow"},"Large Windowed Streaming query"))))),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Note"),": ",(0,r.kt)("em",{parentName:"p"},"You can configure a lot of other advanced configurations related to stream processing(like watermarks) and related to the dagger. Please follow the ",(0,r.kt)("a",{parentName:"em",href:"/dagger/docs/reference/configuration"},"configuration")," section.")),(0,r.kt)("p",null,(0,r.kt)("inlineCode",{parentName:"p"},"Note"),": ",(0,r.kt)("em",{parentName:"p"},"To add a new feature like support for new source/sinks/postprocessors/udfs please follow the ",(0,r.kt)("a",{parentName:"em",href:"/dagger/docs/contribute/contribution"},"contribution guidelines"),".")))}p.isMDXComponent=!0}}]);